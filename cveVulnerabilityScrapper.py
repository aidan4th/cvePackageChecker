#OK HERE is the guide to working with this project. 
# It runs in tandem with a class I made called vendorNode which is used to keep track of the vendors. 


# Here is what the program does. 
# Given a list of packages, with the first collumn being named Package Name and the second being named 
# Version it takes the version from eachone. It needs a version. 
# Than it will look up the database CVEDETAILS to look for known vulnerabilities in the packages. 


# Here are the limitations currently you need to install panda a program to help panda read ods files, 
# beutiful soup and selenuim. Selenuim is more complicated. 
# Once those are installed and the file you want read is in the same directory, 
# you might have to edit the file name reader string. 
# Than you it will type in every version, if thier are more than 50 vendors for a product it will only look at the first 50. 
# If their are more than 50 vulnerabilities per version than it will only look at the first 50. I have never seen this before.

# !!!Secondly it reads the description to see what version the vulnerabilities is talking about. 
# Many applly to a version and the versions before. The program is written to looks for these cases.
# The program gets confused on which numbers are version numbers and just normal numbers. 
# These will be in the * category of vulnerabilities, so read that category and look with your own eyes. 
# Finally many products have multiple vendors and even as a human I could not tell which ones my company was using.
# So instead the program looks through all of them.


#Here are the commands to install everythin.If using python make sure you install it to the
# version you are using. Many operating systems pre install python, but not python 3
# pip install beautifulsoup4
# pip install selenium
# pip install pandas
# pip install odfpy
#
# For selenium you also need to prepare firefox to be run by your computer
# You can use geckodriver for this. In order to install geckodriver you need cargo
# curl https://sh.rustup.rs -sSf | sh
#
# Then install geckodriver with
# cargo install geckodriver
#
# Final note, the version of firefox cannot be the same one that comes with snap
#on ubuntu for all the linux users out their :)
import pandas as pd
from bs4 import BeautifulSoup
from vendorNode import venNode, versionBugs, purifyTest

from  selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys


#Panda is used to read an excel sheet contianing all the possible values. 
#Since I saw we use ODF files at our company I made it read the odf file
#I installed the added ODF package to help with odf files. 
# You must remove the engine odf if you want to use normal excel files
fileInputName = "packageList"
workbook = pd.read_excel(fileInputName + ".ods", engine = "odf", sheet_name = 0)
workbook = workbook[["Package Name", "Version"]]

#Will be used to put data, back into panda data frame. Believe it is easier this way
collectedDataFile = []

#Connects to firefox, you may have to specify route. That was an error I saw online
#But try uninstalling firefox and reinstalling the non snap version first.
driver = webdriver.Firefox()

#This for loop goes through every row of the given spreadsheet 
for workingRow in range(0,len(workbook.index)):
    valuePackageName = str(workbook["Package Name"].iat[workingRow])
    versionNumber = str(workbook["Version"].iat[workingRow])
    if versionNumber.find('+') != -1: versionNumber = versionNumber.split('+')[0] #Cleans up version numbers, many of mine had conversion problem where + was added


    driver.get("https://www.cvedetails.com/product-search.php?vendor_id=0&search=" + valuePackageName)
    websiteContent = driver.page_source
    soup = BeautifulSoup(websiteContent, features="html.parser")


    #What this does is it goes to website searches based on product name then finds all the different
    #Vendors versions of those versions
    vendor = []
    for link in soup.find_all("a"):
        if (link.get("href")[0:8] == "/vendor/"):
            v1 = venNode(link.get_text().lower(), link.get("href").split("/")[2], None, None)
            vendor.append(v1)
    for link in soup.find_all("a"): #Could be combined into 1 for loop for time saving, but I am lazy and that is hard
        if (link.has_attr('href') & link.has_attr('title')):
            if (link.get_text().lower() == valuePackageName and link.get("href")[0:8] != "/vendor/"):
                for ven in vendor:
                    if (ven.id == link.get("href").split("vendor_id=")[1]):
                        ven.linkToV = link.get("href")
                        ven.idP = link.get("href").split("/")[2]

    #I am sorry you had to look at these lines
    #I need an array to create the lines that will be added back into the dataframe from panda
    dataToBeAddedToCollectedDataFile = [None] * 6
    dataToBeAddedToCollectedDataFile[0],dataToBeAddedToCollectedDataFile[1],dataToBeAddedToCollectedDataFile[2],dataToBeAddedToCollectedDataFile[3],dataToBeAddedToCollectedDataFile[4],dataToBeAddedToCollectedDataFile[5] = "Version " + str(versionNumber), "", "Version *","",0,""
    
    
    
    #This goes through every vendor page that we just found
    for ven in vendor:


        countNumberOfLinks = 50 #Counts the number of version links in case we need to go to the next page
        pageNumber = 0
        while(countNumberOfLinks >= 50):
            countNumberOfLinks = 0
            pageNumber += 1

            #Gets website html for the list of versions
            if (ven.linkToV == None): break
            driver.get("https://www.cvedetails.com/version-list/" +  ven.id + "/" +str(ven.idP)+ "/" + str(pageNumber)+ "/" +ven.linkToV.split("/")[3].split("?")[0])
            websiteContent = driver.page_source
            soup = BeautifulSoup(websiteContent, features="html.parser")


            #This loop looks at every version
            for link in soup.find_all("a"):
                versionOfVulnerabilities = link.get("href")
                
                #Sees if the links are a version list
                if (versionOfVulnerabilities[0:20] == "/vulnerability-list/"):
                    countNumberOfLinks += 1 #Increments by 1 to count if page max≈õ out at 50 and the second one should be reached



                    #This will run if the version equals the one we want, lots of string manipulation to find this out
                    if (versionOfVulnerabilities[::-1][5:].split("-")[0][::-1] == versionNumber):
                        
                        #Goes to vulnerability website, to get data
                        driver.get("https://www.cvedetails.com/" + link.get("href"))
                        websiteContent = driver.page_source
                        soupV = BeautifulSoup(websiteContent, features="html.parser")

                        #Reads through every vuklnerability
                        for tdTag in soupV.find_all("td"): #Will not include errors 50+
                            if (tdTag.has_attr('class') & tdTag.has_attr('colspan')):
                                if (tdTag.get('class')[0] == "cvesummarylong"):
                                    tvb = ven
                                    while(tvb.next != None):
                                        tvb = tvb.next
                                    tvb.next =  versionBugs(versionNumber, tdTag.get_text())
                    
                    
                    #This is the null version case, which means the vulnerabilities listed here apply to multiple versions
                    #The purfiyText trys to read this and guess if these versions apply to the version we inputted in the spread sheet
                    elif (versionOfVulnerabilities[::-1][5:][0:1] == "-"):
                        driver.get("https://www.cvedetails.com/" + link.get("href"))
                        websiteContent = driver.page_source
                        soupV = BeautifulSoup(websiteContent, features="html.parser")
                        for tdTag in soupV.find_all("td"): #Will not include errors 50+
                            if (tdTag.has_attr('class') & tdTag.has_attr('colspan')):
                                if (tdTag.get('class')[0] == "cvesummarylong"):
                                    tvb = ven
                                    while(tvb.next != None):
                                        tvb = tvb.next
                                    if (purifyTest(versionNumber, tdTag.get_text())): 
                                        tvb.next =  versionBugs("*", tdTag.get_text())   
        
        
        
        #So The program so fat has taken the list of things and put it into the vb function, now I need to add it to panda.
        # Then the first half should be done. Sorry if this is ugly, but it was the easist way to do it and it makes sense.
        tvb = ven
        while (tvb.next != None):
            tvb = tvb.next
            if (tvb.version == versionNumber):
                dataToBeAddedToCollectedDataFile[4] += 1
                dataToBeAddedToCollectedDataFile[1] = dataToBeAddedToCollectedDataFile[1] + "  [" + ven.vendor + "]"  + tvb.problem.strip()
            else:
                dataToBeAddedToCollectedDataFile[4] += 1
                dataToBeAddedToCollectedDataFile[3] = dataToBeAddedToCollectedDataFile[3] + "  [" + ven.vendor + "] "  + tvb.problem.strip()
        dataToBeAddedToCollectedDataFile[5] = dataToBeAddedToCollectedDataFile[5]+ "[" +ven.vendor+"]"
    collectedDataFile.append(dataToBeAddedToCollectedDataFile)

df = pd.DataFrame(collectedDataFile)
df.columns = ["Version[V]","Vulnerabilities[V]","* Versions","Vulnerabilities[*]","Total Vulnerabilities", "Vendors"]
workbook = workbook.join(df)
workbook.to_csv("packVulList.csv")
driver.quit()